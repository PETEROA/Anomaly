{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh7Es+ONkdBTcidvbxNMtc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PETEROA/Anomaly/blob/main/Spatial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install geopandas pysal\n"
      ],
      "metadata": {
        "id": "_Q0txoWhhVE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVXv6Sy6L1-W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import geopandas as gpd\n",
        "from pysal.explore import esda\n",
        "from pysal.lib import weights\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('air_pollution_final.csv')"
      ],
      "metadata": {
        "id": "TJm5VLMEMyvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install splot"
      ],
      "metadata": {
        "id": "PlanvT-jszZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from splot.esda import moran_scatterplot, lisa_cluster\n",
        "# Assuming you have a DataFrame 'df' with columns 'latitude', 'longitude', 'AQI_1', 'AQI_2', etc.\n",
        "geometry = gpd.points_from_xy(df['longitude'], df['latitude'])\n",
        "gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
        "\n",
        "# Ensure you have a spatial weights matrix\n",
        "w = weights.KNN.from_dataframe(gdf, k=5)\n",
        "\n",
        "# List of variables to analyze\n",
        "variables_to_analyze = ['AQI Value', 'Ozone AQI Value', 'PM2.5 AQI Value']  # Add more variables as needed\n",
        "\n",
        "# Calculate Moran's I for each variable\n",
        "for variable in variables_to_analyze:\n",
        "    moran = esda.Moran(gdf[variable], w)\n",
        "\n",
        "    # Plot Moran Scatterplot\n",
        "    moran_scatterplot(moran, aspect_equal=True)\n",
        "    plt.title(f'Moran Scatterplot for {variable}')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot LISA cluster map\n",
        "    lisa = esda.Moran_Local(gdf[variable], w)\n",
        "    lisa_cluster(lisa, gdf, p=0.05, figsize=(9, 9))\n",
        "    plt.title(f'LISA Cluster Map for {variable}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "P6O2BSCPnZ3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming 'gdf' is your GeoDataFrame\n",
        "coordinates = gdf[['latitude', 'longitude']]\n",
        "\n",
        "# Standardize the features (latitude and longitude) for DBSCAN\n",
        "scaler = StandardScaler()\n",
        "coordinates_scaled = scaler.fit_transform(coordinates)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "gdf['cluster'] = dbscan.fit_predict(coordinates_scaled)\n",
        "\n",
        "# Plot the clusters on a map\n",
        "gdf.plot(column='cluster', categorical=True, legend=True, figsize=(12, 8), cmap='viridis', markersize=10)\n",
        "plt.title('Spatial Clustering with DBSCAN')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rhk083ZixrzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot a histogram of distances\n",
        "plt.hist(distances.max(axis=1), bins=50, edgecolor='black')\n",
        "plt.xlabel('Distance to Nearest Neighbors')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T7Dxx2zT2728"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the distance of each point to its nearest neighbors and consider points with unusually large distances as outliers.\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Define the threshold_distance\n",
        "threshold_distance = 0.02 # Set your threshold value here\n",
        "\n",
        "neighbors = NearestNeighbors(n_neighbors=10)\n",
        "neighbors.fit(gdf[['latitude', 'longitude']])\n",
        "distances, indices = neighbors.kneighbors(gdf[['latitude', 'longitude']])\n",
        "\n",
        "# Consider points with large distances as outliers\n",
        "gdf['max_distance_to_neighbors'] = distances.max(axis=1)\n",
        "gdf['is_outlier_distance'] = gdf['max_distance_to_neighbors'] > threshold_distance\n"
      ],
      "metadata": {
        "id": "nLUmp-2gx6kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have a DataFrame 'gdf' with 'latitude', 'longitude', 'is_outlier_distance'\n",
        "# and 'max_distance_to_neighbors' columns\n",
        "\n",
        "# Select relevant columns\n",
        "features = gdf[['latitude', 'longitude', 'max_distance_to_neighbors']]\n",
        "\n",
        "# Create an Isolation Forest model\n",
        "model = IsolationForest(contamination=0.01)  # Adjust the contamination parameter\n",
        "\n",
        "# Fit the model to the features\n",
        "model.fit(features)\n",
        "\n",
        "# Predict outliers\n",
        "gdf['is_outlier_isolation_forest'] = model.predict(features)\n",
        "\n",
        "# Convert predictions to binary (1 for outliers, -1 for inliers)\n",
        "gdf['is_outlier_isolation_forest'] = np.where(gdf['is_outlier_isolation_forest'] == -1, 1, 0)\n",
        "\n",
        "# Display or use the results as needed\n",
        "print(gdf[['latitude', 'longitude', 'is_outlier_isolation_forest']])\n"
      ],
      "metadata": {
        "id": "cXr56eW23f-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame 'gdf' with 'latitude', 'longitude', and 'max_distance_to_neighbors' columns\n",
        "\n",
        "# Select relevant columns\n",
        "features = gdf[['latitude', 'longitude', 'max_distance_to_neighbors']]\n",
        "\n",
        "# Create a Local Outlier Factor model\n",
        "model = LocalOutlierFactor(contamination=0.05)  # Adjust the contamination parameter\n",
        "\n",
        "# Fit the model to the features\n",
        "model.fit(features)\n",
        "\n",
        "# Predict outliers\n",
        "gdf['lof_scores'] = model.negative_outlier_factor_\n",
        "\n",
        "# Set a threshold for considering points as outliers\n",
        "threshold_lof = -1.5  # Adjust based on your data and experimentation\n",
        "gdf['is_outlier_lof'] = (gdf['lof_scores'] < threshold_lof).astype(int)\n",
        "\n",
        "# Display or use the results as needed\n",
        "print(gdf[['latitude', 'longitude', 'is_outlier_lof']])\n"
      ],
      "metadata": {
        "id": "J0oOJJYk6U0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Assuming you have a DataFrame 'gdf' with 'latitude', 'longitude', and 'max_distance_to_neighbors' columns\n",
        "\n",
        "# Select relevant columns\n",
        "features = gdf[['latitude', 'longitude', 'max_distance_to_neighbors']]\n",
        "\n",
        "# Create a Local Outlier Factor model\n",
        "lof_model = LocalOutlierFactor(contamination=0.05)\n",
        "lof_scores = -lof_model.fit_predict(features)\n",
        "\n",
        "# Create an Isolation Forest model\n",
        "if_model = IsolationForest(contamination=0.02)\n",
        "if_scores = -if_model.fit_predict(features)\n",
        "\n",
        "# Combine scores using average or weighted average\n",
        "ensemble_scores = 0.1 * lof_scores + 0.1 * if_scores  # You can adjust the weights\n",
        "\n",
        "# Set a threshold for considering points as outliers\n",
        "threshold_ensemble = -1.5  # Adjust based on your data and experimentation\n",
        "gdf['is_outlier_ensemble'] = (ensemble_scores < threshold_ensemble).astype(int)\n",
        "\n",
        "# Display or use the results as needed\n",
        "print(gdf[['latitude', 'longitude', 'is_outlier_ensemble']])\n"
      ],
      "metadata": {
        "id": "UOUlgFRG7jVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming you have a column 'is_outlier_ensemble' in your DataFrame\n",
        "y_true = gdf['AQI Value']  # Replace 'true_labels' with actual labels if you have them\n",
        "y_pred = gdf['is_outlier_ensemble']\n",
        "\n",
        "# Precision, Recall, and F1 Score with 'weighted' averaging\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "id": "ktXFX7WEAu5A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}